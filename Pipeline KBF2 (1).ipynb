{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d266830a-b4c6-406a-850f-24fecdb43f59",
   "metadata": {},
   "source": [
    "# KFP V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954cfb47-05c8-4cba-8080-6905b4ea66b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1f65dc99-c13b-4519-84d1-155d0fc394d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "# Ingesta de archivo\n",
    "\n",
    "@dsl.component\n",
    "def cargar_csv(file: str, file2: str, bk: str, rn: str, aws_id: str, aws_secret: str) -> int:\n",
    "    import subprocess\n",
    "    package_name = 'boto3'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    package_name1 = 'pandas'\n",
    "    subprocess.call(['pip', 'install', package_name1])\n",
    "    package_name2 = 'smart_open'\n",
    "    subprocess.call(['pip', 'install', package_name2])\n",
    "    \n",
    "    \n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    from smart_open import open\n",
    "    from os import environ\n",
    "    import io\n",
    "\n",
    "    key = file\n",
    "    region_name = rn  \n",
    "    bucket_name = bk\n",
    "    key_id = aws_id\n",
    "    aws_secret_access_key = aws_secret\n",
    "    \n",
    "    print(\"Region Name \", region_name)\n",
    "    print(\"Bucket Name \", bucket_name)\n",
    "   \n",
    "    s3 = boto3.client('s3', aws_access_key_id = key_id, \n",
    "                        aws_secret_access_key = aws_secret_access_key, \n",
    "                        region_name = region_name)\n",
    "\n",
    "    try:\n",
    "        # Descarga el archivo CSV desde S3\n",
    "        print(\"bucket name:\", bucket_name)\n",
    "        print(\"Key: \", key)\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        print(\"Response: \", response['Body'])\n",
    "        df = pd.read_csv(response['Body'])\n",
    "        if df is not None:\n",
    "            print(\"Archivo CSV cargado exitosamente desde S3 !!!\")\n",
    "\n",
    "            # Guardamos el CSV en el storage S3\n",
    "            bucket_name = bk\n",
    "            file_path = file2\n",
    "            csv_buffer = io.StringIO()\n",
    "            df.to_csv(csv_buffer, index=False)\n",
    "            s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "        \n",
    "            print(\"Archivo CSV copiado al Storage S3 !!!\")\n",
    "            return_code = 0\n",
    "            return return_code\n",
    "        else:\n",
    "            print(\"Error al cargar el archivo CSV desde S3.\")\n",
    "            return_code = 1\n",
    "            return return_code\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error Exception in Python File: {e}\")\n",
    "        return_code = 2\n",
    "        return return_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b56d3a80-103d-4313-b3f4-4c70fdc36a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "\n",
    "@dsl.component(base_image='python:3.11', packages_to_install=['boto3', 'pandas', 'imblearn'])\n",
    "def preprocess(file: str, bk: str, rn: str, aws_id: str, aws_secret: str) -> int:\n",
    "    \n",
    "    import boto3\n",
    "    print(\"boto3\")\n",
    "    import numpy as np\n",
    "    print(\"Numpy\")\n",
    "    import pandas as pd\n",
    "    print(\"Pandas\")\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    print(\"SMOTE\")\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    print(\"KFold\")\n",
    "    from pandas import read_csv\n",
    "    print(\"read csv\")\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    print(\"Robost Scaller\")\n",
    "    import io\n",
    "    print(\"IO\")\n",
    "\n",
    "    key = file\n",
    "    bucket_name = bk\n",
    "    region_name = rn\n",
    "    key_id = aws_id\n",
    "    aws_secret_access_key = aws_secret\n",
    "    \n",
    "    print('Scaling data')\n",
    "    # Leemos el CSV\n",
    "    \n",
    "    s3 = boto3.client('s3', aws_access_key_id = key_id, \n",
    "                        aws_secret_access_key = aws_secret_access_key, \n",
    "                        region_name = region_name)\n",
    "\n",
    "    \n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    df = pd.read_csv(response['Body']) \n",
    "    print(\"File read OK !\")\n",
    "\n",
    "    # Drop de ultima fila\n",
    "    df = df.drop(182328, axis=0)\n",
    "\n",
    "    # Escalamos Amount y Time centrados en 0 (-1, 1)\n",
    "    rob_scaler = RobustScaler()\n",
    "    print(\"Scaler session Ok\")\n",
    "\n",
    "    df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "    print(\"Scale amount OK\")\n",
    "    df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "    print(\"Scale time OK\")\n",
    "\n",
    "\n",
    "    # Eliminamos los Features originales y dejamos los reescalados\n",
    "    df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    scaled_amount = df['scaled_amount']\n",
    "    scaled_time = df['scaled_time']\n",
    "\n",
    "    df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "    df.insert(0, 'scaled_amount', scaled_amount)\n",
    "    df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "\n",
    "    # Armo sets de entrenamiento\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        original_Xtrain = X.iloc[train_index]\n",
    "        original_ytrain = y.iloc[train_index]\n",
    "\n",
    "\n",
    "    # Hacemos SMOTE para balancear los datos...\n",
    "    sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "    Xsm_train, ysm_train = sm.fit_resample(original_Xtrain, original_ytrain)\n",
    "\n",
    "    \n",
    "    # Guardamos el CSV en el storage S3\n",
    "    bucket_name = bk\n",
    "    file_path = 'training_labels.npy'\n",
    "    csv_buffer = io.StringIO()\n",
    "    np.savetxt(csv_buffer, ysm_train, delimiter=\",\")\n",
    "    s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "    \n",
    "    \n",
    "    bucket_name = bk\n",
    "    file_path = 'training_samples.npy'\n",
    "    csv_buffer = io.StringIO()\n",
    "    np.savetxt(csv_buffer, Xsm_train, delimiter=\",\")\n",
    "    s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "    \n",
    "        \n",
    "    print('Data processing done!')\n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "00a69049-e51c-4228-9676-92e480afb386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "@dsl.component(base_image='tensorflow/tensorflow:2.11.0')\n",
    "def training(code: int, bk: str, rn: str, aws_id: str, aws_secret: str, model_name: str) -> int:\n",
    "    import subprocess\n",
    "    package_name = 'boto3'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    package_name = 'boto3'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    package_name = 'tensorflow'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    package_name = 'onnx'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    package_name = 'tf2onnx'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    \n",
    "    from os import environ\n",
    "    import boto3\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import onnx\n",
    " \n",
    "\n",
    "    environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.optimizers import Adam\n",
    "    from numpy import load\n",
    "    from onnx import save\n",
    "    from tf2onnx import convert\n",
    "    import io\n",
    "\n",
    "    print('training model')\n",
    "\n",
    "    epoch_count = 20\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    \n",
    "    bucket_name = bk\n",
    "    region_name = rn\n",
    "    key_id = aws_id\n",
    "    aws_secret_access_key = aws_secret\n",
    "    \n",
    "    \n",
    "    \n",
    "    s3 = boto3.client('s3', aws_access_key_id = key_id, \n",
    "                        aws_secret_access_key = aws_secret_access_key, \n",
    "                        region_name = region_name)\n",
    "\n",
    "  \n",
    "    # Descarga los NPY\n",
    "    key = 'training_labels.npy'\n",
    "    print(\"bucket name:\", bucket_name)\n",
    "    print(\"Key: \", key)\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    \n",
    "    # Leer el contenido del archivo en un buffer\n",
    "    npy_buffer = io.StringIO(response['Body'].read().decode('utf-8'))\n",
    "    # Convertir el buffer a un array de NumPy\n",
    "    ysm_train = np.loadtxt(npy_buffer, delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    key = 'training_samples.npy'\n",
    "    print(\"bucket name:\", bucket_name)\n",
    "    print(\"Key: \", key)\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    # Leer el contenido del archivo en un buffer\n",
    "    npy_buffer = io.StringIO(response['Body'].read().decode('utf-8'))\n",
    "    # Convertir el buffer a un array de NumPy\n",
    "    Xsm_train = np.loadtxt(npy_buffer, delimiter=\",\")\n",
    "    \n",
    "    n_inputs = Xsm_train.shape[1]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(2, activation='sigmoid'),\n",
    "    ])\n",
    "    model.compile(\n",
    "        Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    model.fit(\n",
    "        Xsm_train,\n",
    "        ysm_train,\n",
    "        validation_split=0.2,\n",
    "        batch_size=300,\n",
    "        epochs=epoch_count,\n",
    "        shuffle=True,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    onnx_model, _ = convert.from_keras(model)\n",
    "    \n",
    "    # Convertir el modelo ONNX a bytes en memoria\n",
    "    onnx_buffer = io.BytesIO()\n",
    "    onnx.save_model(onnx_model, onnx_buffer)\n",
    "    onnx_buffer.seek(0)  # Reiniciar el cursor al inicio del flujo de bytes\n",
    "\n",
    "    # Subir el modelo ONNX a S3 directamente desde el flujo de bytes en memoria\n",
    "    s3.put_object(Bucket=bucket_name, Key=model_name, Body=onnx_buffer)\n",
    "\n",
    "   \n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "d4af3ca5-4c02-4602-9dfa-216882bf7163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload del modelo\n",
    "\n",
    "@dsl.component\n",
    "def upload_model(code: int, key: str, key2: str, region_name: str, aws_id: str, aws_secret: str, bk: str) -> int:\n",
    "    import subprocess\n",
    "    package_name = 'boto3'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    package_name = 'pandas'\n",
    "    subprocess.call(['pip', 'install', package_name])\n",
    "    import boto3\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from os import environ\n",
    "\n",
    "    bucket_name = bk\n",
    "    aws_access_key_id = aws_id\n",
    "    aws_secret_access_key = aws_secret\n",
    "    \n",
    "    s3_key_origen = key\n",
    "    s3_key_dest = key2\n",
    "    \n",
    "    print(\"Origen \", s3_key_origen)\n",
    "    print(\"Destino \", s3_key_dest)\n",
    "    print(\"Bucket \", bucket_name)\n",
    "    print(\"Region \", region_name)\n",
    "    \n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, \n",
    "                      aws_secret_access_key=aws_secret_access_key, \n",
    "                      region_name=region_name)\n",
    "\n",
    "    # Leo del S3\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=s3_key_origen)\n",
    "    onnx_model_bytes = response['Body'].read()\n",
    "    \n",
    "   \n",
    "    # Subir el modelo ONNX a otra ubicación en S3\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key_dest, Body=onnx_model_bytes)\n",
    "\n",
    "    print(\"El modelo ONNX se ha guardado correctamente en el bucket de S3 proporcionado.\")\n",
    "    return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "38169739-936f-4c45-b071-6ce62d5e7041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Error\n",
    "\n",
    "@dsl.component\n",
    "def error_component() -> int:\n",
    "    print(\"Error leyando el archivo de datos Origen...\")\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0134f-9987-4428-896a-f67afa1e0185",
   "metadata": {},
   "source": [
    "## Generamos el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "3b15f774-6fec-44df-8014-de5e48828ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dsl.pipeline(name='model-pipeline')\n",
    "def model_pipeline():\n",
    "    import os\n",
    "    from os import environ\n",
    "    \n",
    "    aws_id = environ.get('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret = environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "    \n",
    "    t1_task = cargar_csv(file = 'creditcard.csv', file2 = 'creditcard2.csv', bk = 'semper-pipelines', rn = 'us-east-1', aws_id = aws_id, aws_secret = aws_secret)\n",
    "    with dsl.If(t1_task.output == 0):\n",
    "        t2_task = preprocess(file = 'creditcard2.csv', bk = 'semper-pipelines', rn = 'us-east-1', aws_id = aws_id, aws_secret = aws_secret)\n",
    "        t3_task = training(code = t2_task.output, bk = 'semper-pipelines', rn = 'us-east-1', aws_id = aws_id, aws_secret = aws_secret, model_name = 'model.onnx')\n",
    "        t4_task = upload_model(code = t3_task.output, key = 'model.onnx', key2 = 'data/model-final.onnx', region_name = 'us-east-1',\n",
    "                                                            aws_id = aws_id, aws_secret = aws_secret, bk = 'semper-pipelines')\n",
    "    with dsl.Elif(t1_task.output != 0):\n",
    "        t5_task = error_component()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "744e29ba-8b69-436c-9e2f-02ce50977ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(model_pipeline, 'pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec9671-d460-4e87-acd4-ee46ea52c22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
